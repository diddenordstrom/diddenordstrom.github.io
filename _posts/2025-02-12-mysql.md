---
layout: post
title:  "How we reduced the time of our API SQL-queries by 90%"
date:   2022-05-27 10:00:36 +0200
categories: Learning
author: diddenordstrom
---

# Introduction

Yes, the title is a bit clickbaity – but we really did cut our API SQL query times by 90%.  

To understand how, you first need to know why we were in this situation.  

Over the past three years, my company has built a new, state-of-the-art GraphQL API. It was designed to be the single gateway to our core database, serving external partners, sister companies, and our own internal services all through one unified interface.

Recently an integration partner went live with a high‑volume implementation, and suddenly our API started appearing in the slow query log, with queries taking over 10 seconds. They were hitting an aggregate root that no other client had ever used before. That’s why we hadn’t caught it earlier: the slow query was hiding in plain sight, waiting for the right (or wrong) workload to expose it.


This is the story of how we dug into those slow queries, optimised the worst offenders, and ultimately made our API 10x faster.

# The issue

Queries against our API started appearing in the slow query log, even with our generous threshold of 10 seconds.  
The worst offenders were the initial paginated queries against our main aggregate table. We use that query to fetch a set of root entities (with `ORDER BY id LIMIT 250`), and from there we issue additional `SELECT`s to assemble the full GraphQL object. If the first query is slow, everything downstream suffers. This was unacceptable and needed to be fixed fast.

I began by investigating one particularly slow query, which looked something like this:

```sql
SELECT * 
FROM table_name 
WHERE company_id IN (1, 2, 3) 
  AND archived = false 
ORDER BY id 
LIMIT 250;
```

On the surface, it seems harmless. We even have a composite index containing `company_id` that MySQL could use to narrow down the dataset quickly.
But when I ran `EXPLAIN FORMAT=JSON`, I saw that MySQL was ignoring that index and instead using the primary key (id).


Mmmm, interesting I thought. The obvious index in this case is the `company_id` composite index, but MySQL ignored it all the time. I tried creating a more specific composite index that includes the fields used in the WHERE-statement, change the order of the composite index, remove other ones etc. but MySQL always used the Primary key, no matter how hard I tried. When I forced the index selection in the query directly using hints, the queries where taking < 200ms. So the index was not the problem, the decision to not apply them was. To force an specific index was not really an option as our query builder autogenerated queries. We did not want to write specific cases for each query, and as StackOverflow has told me several times during this research phase, "never ignore index, if you do so, your indexes are faulty or your query is wrong"

Note, my database knowledge is rather limited and I felt out of my depths. So I teamed up with a teammate who has deeper database expertise, and we started digging. We are running an older version of MySQL and we had just read that the MySQL v8 has some improvements with index handling, so naturally we tried bumping it to version 8, just to eliminate any version issues. This was to no avail. My teammate tried the same things I had tried earlier among other things but neither he could crack this puzzle and we where left scratching our heads.
We shared our findings with the whole team and next day we where all focusing on this somewhat strange behaviour. 
After some more research, we had come up with the following:

MySQL’s optimiser balances the cost of filtering against the cost of sorting.
When you ORDER BY id LIMIT 250, the database can either:

- Use the company_id index to find matching rows, then sort them, which requires reading all rows that match the filter and sorting 250 of them.

- Scan the table in primary key order, apply the WHERE clause as it goes, and stop as soon as it has collected 250 matching rows.

MySQL choose the second option. I guess you can start to realize how this can be problematic. What if all the records are located closer to the end of the data set? Then it had to basically perform a full table scan just to get out 250 records.
 

For us in this case, the company that was ending up in the slow queries have regularly been creating entries in this table across many years. To reach 250 records hitting our criteria in the WHERE statement, MySQL had to scan over 2 million rows. We where still unsure how to solve it at this time. Mind you, this query was being run several times a minute and causing a big enough impact we could see the spikes of load in our database monitoring software. Ouch!

After a while, our CTO said, and I quote:

> "This is so stupid, can't we just ignore it?"

# The solution

By "ignore it", he meant adding "IGNORE INDEX" to the statement directly. And whola!

The solution was surprisingly simple: append `IGNORE INDEX FOR ORDER BY (PRIMARY)` to the query.
We added a small regex to our query builder that detected ORDER BY id clauses and automatically injected the hint.

```SQL
SELECT * FROM table_name IGNORE INDEX FOR ORDER BY (PRIMARY) WHERE ...
```

The result? Queries that had been taking ~10 seconds dropped to under 200ms. How's that for an improvement?

## Sources
- https://stackoverflow.com/questions/26048592/why-does-mysql-ignore-index-on-order-by


